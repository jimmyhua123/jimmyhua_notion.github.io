---
layout: post
title: "RLHF ï¼Œpolicy gradient ï¼Œ actor-critic PPOç›¸é—œä»‹ç´¹"
categories: ['NotionExport']
math: true
date: 2025-02-21 10:00:00 +0800
---

### RL ä¸»è¦çµ„ä»¶ï¼š

1. Actorï¼ˆè¡Œç‚ºè€…ï¼‰ï¼šæˆ‘å€‘å¯ä»¥èª¿æ•´çš„éƒ¨åˆ†ï¼Œé€éèª¿æ•´ Policy ä¾†æœ€å¤§åŒ– Rewardã€‚
1. Envï¼ˆç’°å¢ƒï¼‰ï¼šç³»çµ±çµ¦å®šï¼Œç„¡æ³•æ§åˆ¶ã€‚
1. Reward Functionï¼ˆçå‹µå‡½æ•¸ï¼‰ï¼šä¹Ÿæ˜¯äº‹å…ˆçµ¦å®šçš„ï¼Œç„¡æ³•èª¿æ•´ã€‚
å”¯ä¸€å¯èª¿æ•´çš„æ˜¯ Actorï¼Œé€é Policy ä¾†æé«˜ Rewardï¼



---

## ğŸ® RL åœ¨éŠæˆ²ä¸­çš„æ‡‰ç”¨

åœ¨éŠæˆ²ä¸­ï¼Œæ•´å€‹å­¸ç¿’éç¨‹ä¸æ–·å¾ªç’°ï¼Œç›´åˆ°éŠæˆ²çµæŸï¼Œæ¯ä¸€æ¬¡å®Œæ•´çš„éŠæˆ²éç¨‹ç¨±ç‚º ã€ŒEpisodeã€ã€‚

æˆ‘å€‘çš„ç›®æ¨™æ˜¯è®“ Actor æœ€å¤§åŒ–éŠæˆ²éç¨‹ä¸­çš„ Rewardï¼Œå®šç¾©å¦‚ä¸‹ï¼š



R=âˆ‘t=1TrtR = \sum_{t=1}^{T} r_t



ğŸ”¹ Actor å…·æœ‰éš¨æ©Ÿæ€§ï¼Œæˆ‘å€‘åªèƒ½è¨ˆç®—å®ƒçš„æœŸæœ›å€¼ï¼š



RË‰(Î¸)=âˆ‘Ï„R(Ï„)pÎ¸(Ï„)=EÏ„âˆ¼pÎ¸(Ï„)[R(Ï„)]\bar{R}(\theta) = \sum_{\tau} R(\tau) p_{\theta}(\tau) = E_{\tau \sim p_{\theta}(\tau)}\left[R(\tau)\right]



å…¶ä¸­ï¼š

- Ï„ç‚ºä¸€å€‹å¯èƒ½çš„ trajectoryï¼ˆè»Œè·¡ï¼‰ã€‚
- è¨ˆç®—å‡ºæ‰€æœ‰å¯èƒ½çš„ Ï„ å‡ºç¾çš„æ©Ÿç‡ï¼Œä¸¦åŠ æ¬Šå¹³å‡å…¶ total rewardï¼Œå³ç‚ºæœŸæœ›å€¼ã€‚
ğŸ‘‰ æˆ‘å€‘é€é Policy Gradient æ–¹æ³•ä¾†æœ€å¤§åŒ–é€™å€‹æœŸæœ›å€¼ï¼



---

## ğŸ“ˆ Policy Gradient æ–¹æ³•

æ—¢ç„¶æˆ‘å€‘çŸ¥é“æœŸæœ›å€¼çš„è¨ˆç®—æ–¹å¼ï¼Œæˆ‘å€‘å¯ä»¥åˆ©ç”¨ Gradient Ascentï¼ˆæ¢¯åº¦ä¸Šå‡ï¼‰ ä¾†å„ªåŒ– Reward Functionã€‚

ğŸ”¹ ç°¡å–®ä¾†èªªï¼Œå°±æ˜¯åœ¨åƒæ•¸æ›´æ–°æ™‚æ”¹ç‚ºã€ŒåŠ ã€è€Œä¸æ˜¯ã€Œæ¸›ã€ï¼



---

## ğŸ”¥ PPOï¼ˆProximal Policy Optimizationï¼‰

PPO æ˜¯ Policy Gradient æ–¹æ³•çš„é€²ä¸€æ­¥å„ªåŒ–ï¼Œä¸»è¦é€é æ•¸å­¸æ¨å°èˆ‡ Importance Sampling ä¾†å¾—åˆ°ä¸€å€‹æ–°çš„ Objective Functionï¼š



JÎ¸â€²(Î¸)=E(st,at)âˆ¼Ï€Î¸â€²[PÎ¸(atâˆ£st)PÎ¸â€²(atâˆ£st)AÎ¸â€²(st,at)]J^{\theta'}(\theta) = E_{(s_t, a_t) \sim \pi_{\theta'}} \left[ \dfrac{P_\theta(a_t \vert s_t)}{P_{\theta'}(a_t \vert s_t)} A^{\theta'}(s_t, a_t) \right]



é€²ä¸€æ­¥ï¼ŒåŠ å…¥ ç´„æŸé … (constraint)ï¼Œå½¢æˆ PPO ç›®æ¨™å‡½æ•¸ï¼š



JPPOÎ¸â€²(Î¸)=JÎ¸â€²(Î¸)âˆ’Î²KL(Î¸,Î¸â€²)J_{PPO}^{\theta'}(\theta) = J^{\theta'}(\theta) - \beta KL(\theta, \theta')



å…¶ä¸­ï¼š

- PÎ¸(atâˆ£st)PÎ¸â€²(atâˆ£st)\dfrac{P_\theta(a_t \vert s_t)}{P_{\theta'}(a_t \vert s_t)} æ˜¯ é‡è¦æ€§åŠ æ¬Š (Importance Sampling)ï¼Œç¢ºä¿ç­–ç•¥æ›´æ–°æ™‚çš„ç©©å®šæ€§ã€‚
- åŠ å…¥ KL æ•£åº¦ (KL Divergence) ä½œç‚ºæ­£å‰‡åŒ–é …ï¼Œé˜²æ­¢ Policy æ›´æ–°éå¤§ã€‚
ğŸ“Œ é€™æ¨£å¯ä»¥ç¢ºä¿ Actor çš„ç­–ç•¥æ›´æ–°ä¸æœƒåé›¢å¤ªé ï¼Œæå‡å­¸ç¿’çš„ç©©å®šæ€§ï¼



---

### ğŸ–¼ï¸ ç›¸é—œåœ–ç¤ºï¼š

ğŸ“Œ PPO æ–¹æ³•çš„æ¢¯åº¦æ›´æ–°éç¨‹ç¤ºæ„åœ–ï¼š

![image]({{ site.baseurl }}/images/19ffbb85-7f9e-8056-99b9-e201ad4765af.png)

ğŸ“Œ Policy Gradient çš„æ›´æ–°æ–¹å¼ï¼š

![image]({{ site.baseurl }}/images/19ffbb85-7f9e-80dd-a436-c4ce833be2bd.png)



---

## ğŸ ç¸½çµï¼š

- RL ä¸‰å¤§çµ„ä»¶ï¼šActorï¼ˆå¯èª¿æ•´ï¼‰ã€Envï¼ˆä¸å¯æ§ï¼‰ã€Reward Functionï¼ˆä¸å¯æ§ï¼‰ã€‚
- Policy Gradient æ–¹æ³•ï¼šé€éæ¢¯åº¦ä¸Šå‡ä¾†å„ªåŒ–ç­–ç•¥ï¼Œæœ€å¤§åŒ– Rewardã€‚
- PPOï¼šåœ¨ Policy Gradient çš„åŸºç¤ä¸Šï¼ŒåŠ å…¥ Importance Sampling å’Œ KL æ•£åº¦ï¼Œæå‡å­¸ç¿’ç©©å®šæ€§ã€‚
ğŸ¯ é€™äº›æ¦‚å¿µæ˜¯å¼·åŒ–å­¸ç¿’ä¸­æœ€æ ¸å¿ƒçš„å…§å®¹ï¼ŒæŒæ¡å¾Œå°±èƒ½ç†è§£ PPO å¦‚ä½•åœ¨ AI è¨“ç·´ä¸­ç™¼æ®ä½œç”¨ï¼

ğŸ“º åƒè€ƒå½±ç‰‡ï¼š

- [RL èª²ç¨‹å½±ç‰‡ 1](https://www.youtube.com/watch?v=z95ZYgPgXOY&list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_)
- [RL èª²ç¨‹å½±ç‰‡ 3](https://www.youtube.com/watch?v=OAKAZhFmYoI&list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_&index=3)
