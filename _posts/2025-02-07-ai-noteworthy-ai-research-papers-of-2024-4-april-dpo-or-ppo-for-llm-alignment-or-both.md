---
layout: post
title: "4. April: DPO or PPO for LLM alignment, or both?"
categories: ['NotionExport']
math: true
date: 2025-02-19 10:00:00 +0800
---

## 🔗 文獻來源： [DPO Really Better than PPO? Rethinking RLHF](https://arxiv.org/abs/2404.10719)



---

## 🔍 什麼是「對齊」(Alignment)？

對齊（Alignment） 在大型語言模型（LLM）中，指的是使模型的行為與人類的價值觀、偏好一致，確保它能生成符合人類預期的回應。



---

## 🎯 主要演算法介紹

### 1️⃣ PPO（Proximal Policy Optimization，近端策略優化）

- 基於獎勵的強化學習方法，先建立獎勵模型，再透過 actor-critic 方式優化。
- 透過偏好數據（人類標註的「勝出」與「落敗」回應）來訓練獎勵模型。
- 適用於複雜任務，如 程式碼生成，並能適應更具挑戰性的環境。
- 關鍵技術：
  - 優勢標準化（Advantage Normalization）

  - 大批量大小訓練（Large Batch Size）

  - 參考模型的 指數移動平均更新（Exponential Moving Average, EMA）

- 目標函數：
  $$
J_r(\pi_\theta) = \mathbb{E}{x \sim p{\text{data}}, y \sim \pi_\theta}
\left[ r(x,y) - \beta \log \left( \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)} \right) \right]
$$


  


---

### 2️⃣ DPO（Direct Preference Optimization，直接偏好優化）

- 無獎勵的強化學習方法，直接基於偏好數據來優化策略，不需要額外的獎勵模型。
- 目標是簡化訓練流程，從人類偏好中學習。
- 但 對分佈偏移（Distribution Shift）敏感，如果測試資料與訓練資料分佈不同，效果可能較差。
- 改進方法：
  - DPO-Iter（迭代 DPO）：透過生成新回應並使用獎勵模型標記，以減少偏移影響。



---

## 🔬 PPO 與 DPO 的比較

| 特性 | PPO | DPO |
| --- | --- | --- |
| 方法類型 | 基於獎勵（需要獎勵模型） | 無獎勵（直接學習偏好） |
| 獎勵模型 | 需要訓練 | 不需要 |
| 分佈偏移敏感性 | 低 | 高 |
| 適用性 | 適用於程式碼生成等高難度任務 | 適用於分佈內數據，但分佈外數據效果不佳 |
| 訓練複雜度 | 高（需額外訓練獎勵模型） | 低（無需額外獎勵模型） |
| 改進技術 | 優勢標準化、大批量訓練、指數移動平均 | DPO-Iter（迭代優化） |



---



## 📊 研究重點與發現

### 🔹 研究問題

- DPO 真的優於 PPO 嗎？
- 為什麼 PPO 在某些 RLHF 基準測試中表現不佳？
- 如何增強 PPO 的實際性能？
### 🔍 研究方法

1. 理論分析：分析 DPO 的根本限制，探索 PPO 增強方法。
1. 消融研究：測試 PPO 的不同組件，確定最佳訓練策略。
1. 廣泛實驗：
  - 對話生成

  - 程式碼生成

  - 不同回饋類型與難度的測試數據集



---

## 🔑 主要發現

### ✅ PPO 在多數情況下優於 DPO

- 在所有實驗中，PPO 始終優於 DPO，尤其在 程式碼競賽 任務中，PPO 取得 SOTA（最先進） 的結果。
### ⚠️ DPO 的局限性

1. 可能找到偏差解：DPO 容易受到 分佈外數據影響，可能導致無法預測的行為。
1. 對分佈偏移敏感：當測試數據與訓練數據不同時，DPO 的效果可能顯著下降。
1. 程式碼生成表現不佳：即使訓練數據標註接近完美，DPO 仍無法有效提升程式碼生成能力。
### 🎯 PPO 提升效果的關鍵

- 優勢正規化（Advantage Normalization）：穩定訓練並提升效果。
- 大批量大小（Large Batch Size）：適用於 程式碼生成 等高難度任務。
- 參考模型的指數移動平均更新（Ref. EMA）：提高 PPO 在 RLHF 內的穩定性。


---

## 🏆 實驗結果比較

| 指標 | PPO | DPO |
| --- | --- | --- |
| 總體性能 | 通常優於 DPO | 某些情況下有效，但整體不如 PPO |
| 分佈外數據適應性 | 較強，可處理偏好數據外的回應 | 對分佈偏移敏感，易受影響 |
| 程式碼生成能力 | 表現極佳 | 效果不佳 |
| 實施難度 | 需訓練獎勵模型，較複雜 | 簡單，計算效率高 |
| 應用場景 | 使用於 Llama 2，適用於多種複雜任務 | 使用於 Llama 3，適用於簡單 RLHF 調整 |



---

## 🎯 總結

- PPO 在 RLHF 中表現更穩定且強大，尤其適用於 程式碼生成 和 複雜回應生成。
- DPO 雖然實施簡單，但對數據分佈較敏感，在某些情況下效果不佳。
- DPO-Iter（迭代 DPO）能緩解部分問題，但仍無法在高難度任務上超越 PPO。
- PPO 在 RLHF 訓練中具有更好的適應性與泛化能力，是更可靠的選擇。
📌 結論：DPO 在計算上更具效率（不需要訓練和使用單獨的獎勵模型），因此在實務上較常見，但 若需高效能表現（特別是程式碼生成等挑戰性任務），PPO 更為優秀！ 🚀
