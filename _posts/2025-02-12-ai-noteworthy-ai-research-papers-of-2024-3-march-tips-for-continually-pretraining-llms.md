---
layout: post
title: "3. March: Tips for Continually Pretraining LLMs"
categories: ['NotionExport']
math: true
date: 2025-02-13 10:00:00 +0800
---

## 🔗 文獻來源：[arXiv:2403.08763](https://arxiv.org/abs/2403.08763)



---



# 📚 持續預訓練 (Continual Pre-training) 的概念與挑戰

## 🌟 什麼是持續預訓練？

- 大型語言模型 (LLM) 的訓練成本極高，並需要持續更新以維持競爭力。
- 傳統做法是使用舊數據與新數據重新訓練整個模型，但這非常耗費資源。
- 持續預訓練 (Continual Pre-training) 提供了一種更高效的替代方案，它不從零開始，而是基於現有模型繼續訓練，使其適應新數據。
## 🔍 預訓練 (Pre-training) 的基本概念

- 預訓練 指的是在大規模未標記數據上訓練模型，使其學習通用的語言特徵與表示。
- 預訓練後，模型可以用於各種下游任務，並透過微調 (fine-tuning) 來適應特定需求。
## ⚠️ 持續預訓練的挑戰

持續預訓練可能帶來以下問題：

1. 適應不良 (Poor Adaptation)：模型無法有效學習新數據。
1. 災難性遺忘 (Catastrophic Forgetting)：模型在舊數據上的能力顯著下降。
🔹 解決方案：

- 學習率的重新加溫 (re-warming) 與衰減 (re-decaying)。
- 重播 (replay) 先前的數據，避免模型遺忘舊知識。


---

# 🚀 解決方案與技術策略

## 1️⃣ 學習率的重新加溫與衰減

🔹 為何需要重新加溫？

- 在初次訓練時，學習率會逐漸降低，以穩定模型的學習。
- 持續預訓練時，需要提高學習率 (re-warm)，然後再逐漸衰減 (re-decay)，以適應新數據。
🔹 學習率調整的影響

- 提高學習率上限 (ηmax) → 提高適應性，但可能增加遺忘風險。
- 降低學習率上限 → 減少遺忘，但可能影響適應新數據的能力。
- 線性加溫 (Linear Warmup) 階段的長度 對遺忘與適應影響不大。
## 2️⃣ 數據重播 (Replay)

🔹 核心概念

- 在訓練新數據時，同時使用一部分舊數據，有助於保留舊知識。
- 即使重播比例僅 1% - 5%，也能有效減少災難性遺忘。
🔹 重播比例的影響

- 少量重播 (1% - 5%) → 平衡適應與遺忘，效果最佳。
- 過多重播 (50%) → 可能降低模型對新數據的適應能力。
📌 建議： 透過實驗找出最適合的重播比例，以取得最佳平衡點！



---

# 🔬 其他策略與研究發現

## 3️⃣ Cosine 學習率排程

- 機制：學習率按照餘弦函數曲線衰減，是 LLM 訓練中的常見策略。
- 應用：許多開源 LLM 預訓練均採用此方法。
## 4️⃣ 無限學習率排程 (Infinite Learning Rate Schedules)

- 機制：這種方法避免了學習率重新加溫可能導致的遺忘。
- 組成：包括線性加溫階段、恆定學習率階段與退火 (annealing) 階段。
- 優勢：
  - 在不同訓練階段保持穩定學習率，減少遺忘風險。

  - 訓練可靈活調整，不受預設 token 數量限制。

## 5️⃣ 優化器狀態重置

- 做法：當模型切換到新數據時，重置優化器狀態 (Optimizer State Reset)。
- 發現：實驗顯示，是否重置優化器對模型效能影響不大。
## 6️⃣ 模型擴展 (Model Growing)

- 方法：在持續預訓練時增加模型規模，例如：
  - 混合專家模型 (Mixture-of-Experts, MoE)

  - 模塊擴展 (Block Expansion)

- 目的：允許模型學習新知識的同時，保留舊知識。
## 7️⃣ Tokenizer 調整

- 機制：當數據分布發生劇變時，調整 tokenizer 來適應新數據。
- 目的：確保模型能夠有效處理新的語言結構與詞彙。
## 8️⃣ 模型合併 (Model Merging)

- 機制：將單獨訓練的模型合併，例如 TIES 方法。
- 比較：研究發現，持續預訓練的效果通常優於模型合併，但所需計算資源相當。
## 9️⃣ 針對特定領域的持續預訓練 (DACPT)

- 機制：針對不同領域的數據，進行自監督學習，並保持在各領域的表現。
- 比較：本文主要探討的是通用數據集，而非特定領域的數據集。


---

# 🔑 研究結論

✅ 持續預訓練可顯著降低計算成本，並維持模型性能。

✅ 學習率的重新加溫與衰減是關鍵，影響模型的適應與遺忘。

✅ 即使數據分布劇烈變化 (如英語→德語)，適當調整學習率與重播數據，仍可達到與從零訓練相當的表現。

✅ 該方法適用於不同模型大小 (405M ~ 10B 參數)，顯示其適用性廣泛。



---
